{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First combine the lists of raw names from each website scraped with BeautifulSoup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9332"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = []\n",
    "for rx_site in ['assist', 'list']:\n",
    "    with open(f'raw/names_rx{rx_site}.json', 'r') as f:\n",
    "        names += list(json.load(f))\n",
    "len(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9332 total names. Looking at the contents of the json files there are clearly some duplicates especially when the names involve two words. Generally anything beyond the first word isn't necessarily relevant to the original brand name itself. \n",
    "\n",
    "The following saves only the first word for names and removes duplicates. It also makes names all lowercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3704"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_name(name_str):\n",
    "    name_str = name_str.split(' ')[0]\n",
    "    name_str = name_str.lower()\n",
    "    return name_str\n",
    "\n",
    "names_clean = list(map(clean_name, names))  # apply the element-wise function\n",
    "names_clean = list(set(names_clean))        # converting to a set and back to a list will remove duplicates\n",
    "len(names_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this step only 3704 names remain. Further filtering may be unwise at this point since we want a sufficiently large dataset. \n",
    "\n",
    "The following checks which characters appear in the names other than from the standard English alphabet, to see if there are any special characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',', '1', '6', '2', '3', '4', '\\t', '5', '8', '7', '.', '0', 'é', '/', '-']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_str_long = ''.join(names_clean)\n",
    "chars = list(\n",
    "    set(names_str_long).difference(\n",
    "        set(string.ascii_lowercase)\n",
    "    )\n",
    ")\n",
    "chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at these suspect names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words with \",\":\n",
      "['naprosyn,', 'glucophage,', 'prempro,', 'aerobid,', 'biaxin,']\n",
      "--------\n",
      "Words with \"1\":\n",
      "['hominex-1', 'b12', 'propimex-1', 'vagistat-1', 'i-valex-1', 'glutarex-1', 'niferex-150', 'glofil-125', 'phenex-1', 'cnj-016', 'cyclinex-1', 'ketonex-1', 'tyrex-1']\n",
      "--------\n",
      "Words with \"6\":\n",
      "['cnj-016', 'md-76r']\n",
      "--------\n",
      "Words with \"2\":\n",
      "['b12', 'podocon-25', 'i-valex-2', 'ketonex-2', 'tyrex-2', 'cyclinex-2', 'hominex-2', 'glutarex-2', 'acam2000', 'theo-24', 'nordette-28', 'glofil-125', 'mdp-25', 'phenex-2', 'cardiogen-82', 'propimex-2', 'trivora-28']\n",
      "--------\n",
      "Words with \"3\":\n",
      "['gelsyn-3', 'melquin-3']\n",
      "--------\n",
      "Words with \"4\":\n",
      "['neotrace-4', 'theo-24', 'kenalog-40']\n",
      "--------\n",
      "Words with \"\t\":\n",
      "['voraxaze\\t']\n",
      "--------\n",
      "Words with \"5\":\n",
      "['anadrol-50', 'podocon-25', 'niferex-150', 'glofil-125', 'mdp-25', 'rimso-50']\n",
      "--------\n",
      "Words with \"8\":\n",
      "['8-mop', 'nordette-28', 'cardiogen-82', 'trivora-28']\n",
      "--------\n",
      "Words with \"7\":\n",
      "['md-76r']\n",
      "--------\n",
      "Words with \".\":\n",
      "['d.', 'h.p.', 'e.e.s.']\n",
      "--------\n",
      "Words with \"0\":\n",
      "['anadrol-50', 'acam2000', 'niferex-150', 'kenalog-40', 'cnj-016', 'rimso-50']\n",
      "--------\n",
      "Words with \"é\":\n",
      "['juvéderm']\n",
      "--------\n",
      "Words with \"/\":\n",
      "['derma-smoothe/fs']\n",
      "--------\n",
      "Words with \"-\":\n",
      "['low-ogestrel', 'mono-vacc', 'gelsyn-3', 'nitro-dur', 'klor-con', 'readi-cat', 'dilaudid-hp', 'alka-seltzer', 'olux-e', 'dtic-dome', 'vivelle-dot', 'signifor-lar', 'lo-zumandimine', 'monoclate-p', 'dyna-hex', 'regen-cov', 'auvi-q', 'hominex-1', 'r-gene', 'anadrol-50', 'tri-sprintec', 'depo-subq', 'thyro-tabs', 'ms-contin', 'nor-qd', 'm-r-vax', 'nabi-hb', 'monistat-derm', 'adipex-p', 'ez-disk', 'podocon-25', 'gel-one', 'i-valex-2', 'a-methapred', 'ketonex-2', 'yf-vax', 'propimex-1', 'gonal-f', 'depo-provera', 'urocit-k', 'k-tab', 'tofranil-pm', 'tyrex-2', 'cyclinex-2', 'je-vax', 'tri-linyah', 'depo-estradiol', 'lac-hydrin', 'hominex-2', 'roferon-a', 'qmiiz-odt', '8-mop', 'zembrace-symtouch', 'poly-vi-flor', 'dritho-scalp', 'e-z-hd', 'autoplex-t', 'covera-hs', 'gavilyte-g', 'hydro-q', 'phenergan-codeine', 'beconase-aq', 'k-phos', 'vagistat-1', 'i-valex-1', 'allegra-d', 'glutarex-1', 'neotrace-4', 'gavilyte-c', 'lumi-sporyn', 'engerix-b', 'md-gastroview', 'hi-cal', 'zn-dtpa', 'neo-synalar', 'ca-dtpa', 'an-sulfur', 'glutarex-2', 'entex-t', 'gyne-lotrimin', 'isovue-m', 'depo-medrol', 'theo-24', 'ak-fluor', 'neo-synephrine', 'aci-jel', 'oxsoralen-ultra', 'hemofil-m', 'clarinex-d', 'restylane-l', 'niferex-150', 'neosporin-gu', 'k-lor', 'thrombin-jmi', 'ic-green', 'neo-fradin', 'mono-linyah', 'derma-smoothe', 'hep-lock', 'levo-t', 'miochol-e', 'gavilyte-n', 'nora-be', 'plasma-lyte', 'darvocet-n', 'ortho-cept', 'glyrx-pf', 'flo-pred', 'terra-cortril', 'nordette-28', 'glofil-125', 'mdp-25', 'proplex-t', 'phenex-1', 'm-m-r', 'catapres-tts', 'ak-pentolate', 'phenex-2', 'kenalog-40', 'tuxarin-er', 'platinol-aq', 'ana-kit', 'depo-testosterone', 'cardiogen-82', 'cnj-016', 'zyrtec-d', 'peg-intron', 'ery-tab', 'gamunex-c', 'md-76r', 'tylenol-codeine', 'derma-smoothe/fs', 'melquin-3', 'tev-tropin', 'cyclinex-1', 'alphagan-p', 'ketonex-1', 'humate-p', 'ruby-fill', 'propimex-2', 'ortho-novum', 'timoptic-xe', 'slow-k', 'slo-phyllin', 'rimso-50', 'omeclamox-pak', 'halog-e', 'micro-k', 'pred-g', 'coly-mycin', 'poly-pred', 'epivir-hbv', 'vira-a', 'cis-sulfur', 'retin-a', 'tri-luma', 'trivora-28', 'center-al', 'ultra-technekow', 'tyrex-1', 'wp-thyroid', 'chlor-trimeton', 'perlane-l', 'tirosint-sol', 'cytra-k', 'synvisc-one', 'nature-throid', 'np-thyroid', 'gavilyte-h', 'paxil-cr']\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "for char in chars:\n",
    "    print(f'Words with \"{char}\":')\n",
    "    print([name for name in names_clean if char in name])\n",
    "    print('--------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems more duplicates need be removed since the hyphen often acts as an alternative for a space when adding descriptors to the name. To try to best account for the inconcistency of format of the hyphenated names, since sometimes the descriptor tags come before (e.g. `tri-sprintec`) or after (e.g. `neotrace-4`) the presumed drug name, we will assume that the actual drug name in a sequence of hyphenated strings is the largest substring in the sequence.\n",
    "\n",
    "So, in the previous examples, we would extract the names `sprintec` and `neotrace`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hyphens(name_str):\n",
    "    substrings = name_str.split('-')\n",
    "    longest = max(substrings, key=len)\n",
    "    return longest\n",
    "\n",
    "names_clean = list(map(clean_hyphens, names_clean))     # apply the element-wise function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what outlier words remain with similar code from earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words with \",\": ['naprosyn,', 'glucophage,', 'prempro,', 'aerobid,', 'biaxin,']\n",
      "Words with \"1\": ['b12']\n",
      "Words with \"6\": ['76r']\n",
      "Words with \"2\": ['b12', 'acam2000']\n",
      "Words with \"\t\": ['voraxaze\\t']\n",
      "Words with \"7\": ['76r']\n",
      "Words with \".\": ['d.', 'h.p.', 'e.e.s.']\n",
      "Words with \"0\": ['acam2000']\n",
      "Words with \"é\": ['juvéderm']\n",
      "Words with \"/\": ['smoothe/fs']\n"
     ]
    }
   ],
   "source": [
    "names_str_long = ''.join(names_clean)\n",
    "chars = list(\n",
    "    set(names_str_long).difference(\n",
    "        set(string.ascii_lowercase)\n",
    "    )\n",
    ")\n",
    "\n",
    "for char in chars:\n",
    "    print(f'Words with \"{char}\":', [name for name in names_clean if char in name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make our final cleanup edits based on these words as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_misc(name_str):\n",
    "    name_str = name_str.strip()     # handle the \\t tab character\n",
    "    name_str = name_str.replace(',', '')       # remove commas from names\n",
    "    name_str = name_str.replace('é', 'e')      # remove accent on the single name\n",
    "    return name_str\n",
    "\n",
    "names_clean = list(map(clean_misc, names_clean))     # apply the element-wise function\n",
    "\n",
    "# Now remove the rest of the remaining outlier words\n",
    "names_str_long = ''.join(names_clean)\n",
    "chars = list(\n",
    "    set(names_str_long).difference(\n",
    "        set(string.ascii_lowercase)\n",
    "    )\n",
    ")\n",
    "\n",
    "outlier_words = []\n",
    "for char in chars:\n",
    "    outlier_words += [name for name in names_clean if char in name]\n",
    "\n",
    "names_clean = list(set(names_clean).difference(set(outlier_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we should probably remove names that are excessively short, and make sure we remove any duplicates generated by the cleaning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3601"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_clean = [name for name in names_clean if len(name) > 3]\n",
    "names_final = list(set(names_clean))\n",
    "len(names_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 3601 total names to work with. Let's export them to a final JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('names_clean.json', 'w') as f:\n",
    "    json.dump(names_final, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ML-PyTorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8557bf458b9afab58327ce3742e66e2ee128b7519b8bbb9491e9b1b2bd2599ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
