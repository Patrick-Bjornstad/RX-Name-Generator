{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First combine the lists of raw names from each website scraped with BeautifulSoup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9332"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = []\n",
    "for rx_site in ['assist', 'list']:\n",
    "    with open(f'raw/names_rx{rx_site}.json', 'r') as f:\n",
    "        names += list(json.load(f))\n",
    "len(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9332 total names. Looking at the contents of the json files there are clearly some duplicates especially when the names involve two words. Generally anything beyond the first word isn't necessarily relevant to the original brand name itself. \n",
    "\n",
    "The following saves only the first word for names and removes duplicates. It also makes names all lowercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3704"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_name(name_str):\n",
    "    name_str = name_str.split(' ')[0]\n",
    "    name_str = name_str.lower()\n",
    "    return name_str\n",
    "\n",
    "names_clean = list(map(clean_name, names))  # apply the element-wise function\n",
    "names_clean = list(set(names_clean))        # converting to a set and back to a list will remove duplicates\n",
    "len(names_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this step only 3704 names remain. Further filtering may be unwise at this point since we want a sufficiently large dataset. \n",
    "\n",
    "The following checks which characters appear in the names other than from the standard English alphabet, to see if there are any special characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '\\t', 'é', '0', '6', '-', '8', '4', '5', '.', '/', '7', ',', '2', '3']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_str_long = ''.join(names_clean)\n",
    "chars = list(\n",
    "    set(names_str_long).difference(\n",
    "        set(string.ascii_lowercase)\n",
    "    )\n",
    ")\n",
    "chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at these suspect names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words with \"1\":\n",
      "['propimex-1', 'cyclinex-1', 'glofil-125', 'ketonex-1', 'cnj-016', 'niferex-150', 'vagistat-1', 'tyrex-1', 'hominex-1', 'phenex-1', 'i-valex-1', 'b12', 'glutarex-1']\n",
      "--------\n",
      "Words with \"\t\":\n",
      "['voraxaze\\t']\n",
      "--------\n",
      "Words with \"é\":\n",
      "['juvéderm']\n",
      "--------\n",
      "Words with \"0\":\n",
      "['cnj-016', 'niferex-150', 'rimso-50', 'anadrol-50', 'kenalog-40', 'acam2000']\n",
      "--------\n",
      "Words with \"6\":\n",
      "['cnj-016', 'md-76r']\n",
      "--------\n",
      "Words with \"-\":\n",
      "['propimex-1', 'ery-tab', 'neo-synalar', 'phenergan-codeine', 'np-thyroid', 'platinol-aq', 'gelsyn-3', 'melquin-3', 'cyclinex-1', 'gavilyte-g', 'derma-smoothe/fs', 'levo-t', 'glofil-125', 'ketonex-1', 'cyclinex-2', 'zyrtec-d', 'tylenol-codeine', 'hydro-q', 'dilaudid-hp', 'micro-k', 'hi-cal', 'retin-a', 'cnj-016', 'omeclamox-pak', 'gavilyte-c', 'lo-zumandimine', 'thyro-tabs', 'ez-disk', 'poly-pred', 'tyrex-2', 'gamunex-c', 'cardiogen-82', 'lac-hydrin', 'r-gene', 'glyrx-pf', 'halog-e', 'niferex-150', 'gavilyte-n', 'timoptic-xe', 'paxil-cr', 'vagistat-1', 'ultra-technekow', 'autoplex-t', 'depo-provera', 'neotrace-4', 'allegra-d', 'an-sulfur', 'coly-mycin', 'humate-p', 'chlor-trimeton', 'peg-intron', 'mono-vacc', 'dyna-hex', 'zn-dtpa', 'mdp-25', 'ic-green', 'nordette-28', 'tirosint-sol', 'wp-thyroid', 'auvi-q', 'urocit-k', 'tyrex-1', 'depo-testosterone', 'derma-smoothe', 'neo-fradin', 'thrombin-jmi', 'nor-qd', 'ana-kit', 'alka-seltzer', 'propimex-2', 'tri-sprintec', 'cis-sulfur', 'vira-a', 'klor-con', 'k-tab', 'aci-jel', 'oxsoralen-ultra', 'hominex-1', 'depo-medrol', 'low-ogestrel', 'monistat-derm', 'mono-linyah', 'pred-g', 'olux-e', 'epivir-hbv', 'yf-vax', 'vivelle-dot', 'phenex-1', 'i-valex-1', 'ketonex-2', 'signifor-lar', 'neo-synephrine', 'md-76r', 'md-gastroview', 'engerix-b', 'readi-cat', 'lumi-sporyn', 'ca-dtpa', 'e-z-hd', 'nora-be', 'rimso-50', 'beconase-aq', 'nabi-hb', 'poly-vi-flor', 'synvisc-one', 'tofranil-pm', 'covera-hs', 'depo-subq', 'ortho-cept', 'podocon-25', '8-mop', 'dtic-dome', 'tri-luma', 'miochol-e', 'je-vax', 'i-valex-2', 'hemofil-m', 'anadrol-50', 'phenex-2', 'regen-cov', 'hominex-2', 'dritho-scalp', 'nitro-dur', 'plasma-lyte', 'ruby-fill', 'nature-throid', 'alphagan-p', 'neosporin-gu', 'gel-one', 'gyne-lotrimin', 'center-al', 'gavilyte-h', 'darvocet-n', 'tuxarin-er', 'entex-t', 'perlane-l', 'kenalog-40', 'ak-pentolate', 'slow-k', 'trivora-28', 'ms-contin', 'monoclate-p', 'a-methapred', 'depo-estradiol', 'hep-lock', 'cytra-k', 'k-phos', 'k-lor', 'glutarex-2', 'proplex-t', 'ortho-novum', 'catapres-tts', 'gonal-f', 'terra-cortril', 'flo-pred', 'm-m-r', 'ak-fluor', 'roferon-a', 'm-r-vax', 'glutarex-1', 'zembrace-symtouch', 'adipex-p', 'tri-linyah', 'qmiiz-odt', 'slo-phyllin', 'clarinex-d', 'isovue-m', 'tev-tropin', 'restylane-l', 'theo-24']\n",
      "--------\n",
      "Words with \"8\":\n",
      "['cardiogen-82', 'nordette-28', '8-mop', 'trivora-28']\n",
      "--------\n",
      "Words with \"4\":\n",
      "['neotrace-4', 'kenalog-40', 'theo-24']\n",
      "--------\n",
      "Words with \"5\":\n",
      "['glofil-125', 'niferex-150', 'mdp-25', 'rimso-50', 'podocon-25', 'anadrol-50']\n",
      "--------\n",
      "Words with \".\":\n",
      "['d.', 'h.p.', 'e.e.s.']\n",
      "--------\n",
      "Words with \"/\":\n",
      "['derma-smoothe/fs']\n",
      "--------\n",
      "Words with \"7\":\n",
      "['md-76r']\n",
      "--------\n",
      "Words with \",\":\n",
      "['aerobid,', 'prempro,', 'glucophage,', 'naprosyn,', 'biaxin,']\n",
      "--------\n",
      "Words with \"2\":\n",
      "['glofil-125', 'cyclinex-2', 'tyrex-2', 'cardiogen-82', 'mdp-25', 'nordette-28', 'propimex-2', 'ketonex-2', 'podocon-25', 'i-valex-2', 'phenex-2', 'hominex-2', 'trivora-28', 'acam2000', 'glutarex-2', 'b12', 'theo-24']\n",
      "--------\n",
      "Words with \"3\":\n",
      "['gelsyn-3', 'melquin-3']\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "for char in chars:\n",
    "    print(f'Words with \"{char}\":')\n",
    "    print([name for name in names_clean if char in name])\n",
    "    print('--------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems more duplicates need be removed since the hyphen often acts as an alternative for a space when adding descriptors to the name. To try to best account for the inconcistency of format of the hyphenated names, since sometimes the descriptor tags come before (e.g. `tri-sprintec`) or after (e.g. `neotrace-4`) the presumed drug name, we will assume that the actual drug name in a sequence of hyphenated strings is the largest substring in the sequence.\n",
    "\n",
    "So, in the previous examples, we would extract the names `sprintec` and `neotrace`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hyphens(name_str):\n",
    "    substrings = name_str.split('-')\n",
    "    longest = max(substrings, key=len)\n",
    "    return longest\n",
    "\n",
    "names_clean = list(map(clean_hyphens, names_clean))     # apply the element-wise function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what outlier words remain with similar code from earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words with \"1\":\n",
      "['b12']\n",
      "--------\n",
      "Words with \"é\":\n",
      "['juvéderm']\n",
      "--------\n",
      "Words with \"0\":\n",
      "['acam2000']\n",
      "--------\n",
      "Words with \"6\":\n",
      "['76r']\n",
      "--------\n",
      "Words with \".\":\n",
      "['d.', 'h.p.', 'e.e.s.']\n",
      "--------\n",
      "Words with \"/\":\n",
      "['smoothe/fs']\n",
      "--------\n",
      "Words with \"7\":\n",
      "['76r']\n",
      "--------\n",
      "Words with \",\":\n",
      "['aerobid,', 'prempro,', 'glucophage,', 'naprosyn,', 'biaxin,']\n",
      "--------\n",
      "Words with \"2\":\n",
      "['acam2000', 'b12']\n",
      "--------\n",
      "Words with \"\t\":\n",
      "['voraxaze\\t']\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "names_str_long = ''.join(names_clean)\n",
    "chars = list(\n",
    "    set(names_str_long).difference(\n",
    "        set(string.ascii_lowercase)\n",
    "    )\n",
    ")\n",
    "\n",
    "for char in chars:\n",
    "    print(f'Words with \"{char}\":')\n",
    "    print([name for name in names_clean if char in name])\n",
    "    print('--------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO: remove too short names, (probably) anything with a number or \".\", and simply mask out the rest of the characters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "adaeca8c19b5cfc3eefe2ab2bb3bcd51a4a0f8192995e4561f02feb302611800"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
